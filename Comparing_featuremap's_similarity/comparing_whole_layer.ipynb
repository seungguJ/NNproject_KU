{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the whole feature maps not only last layer\n",
    "\n",
    "How about simply modify the first layer or the layer that difference is getting bigger?\n",
    "\n",
    "So check the whole layers' feature maps when the original image and perturbed image get into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_grad_cam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Comparing_featuremap's_similarity/comparing_whole_layer.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73775f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f70726d6c403136332e3135322e37342e3735227d7d/workspace/Comparing_featuremap%27s_similarity/comparing_whole_layer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73775f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f70726d6c403136332e3135322e37342e3735227d7d/workspace/Comparing_featuremap%27s_similarity/comparing_whole_layer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73775f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f70726d6c403136332e3135322e37342e3735227d7d/workspace/Comparing_featuremap%27s_similarity/comparing_whole_layer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_grad_cam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m show_cam_on_image\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73775f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f70726d6c403136332e3135322e37342e3735227d7d/workspace/Comparing_featuremap%27s_similarity/comparing_whole_layer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m resnet50\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73775f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f70726d6c403136332e3135322e37342e3735227d7d/workspace/Comparing_featuremap%27s_similarity/comparing_whole_layer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPGD\u001b[39;00m \u001b[39mimport\u001b[39;00m PGD\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_grad_cam'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet50\n",
    "from PGD import PGD\n",
    "from my_resnet import *\n",
    "from models import *\n",
    "import os\n",
    "import torchvision\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\"  # Set the GPUs 1 and 2 to use\n",
    "    \n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(cuda)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='/workspace/my_data', train=False, download=False, transform=transform_test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "model = ResNet18() ## original model\n",
    "checkpoint_path2 = '/workspace/checkpoint/resnet18_cifar10'\n",
    "checkpoint2 = torch.load(checkpoint_path2)['net']\n",
    "for key in list(checkpoint2.keys()):\n",
    "    checkpoint2[key.replace('module.', '')] = checkpoint2.pop(key)\n",
    "model.load_state_dict(checkpoint2, strict=False)## original model\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "attack = PGD(model, cuda=cuda) # original model\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "max_dict = {}\n",
    "min_dict = {}\n",
    "\n",
    "for ori_imgs, ori_labels in test_loader:\n",
    "\n",
    "    pgd_imgs = attack(ori_imgs, ori_labels)\n",
    "    ori_imgs = ori_imgs.cuda()\n",
    "\n",
    "    # output_dict = {}\n",
    "    \n",
    "    # def forwardhooking(layer_name):\n",
    "    #     def hook(m, inp, out):\n",
    "    #         output_dict[layer_name] = out\n",
    "    #     return hook\n",
    "    holder = []\n",
    "    \n",
    "    def forwardhooking(m, input, output):\n",
    "        holder.append(output)\n",
    "    \n",
    "    for name, m in model.named_modules():\n",
    "        if \"conv\" in name or \"bn\" in name or \"linear\" in name:\n",
    "            m.register_forward_hook(forwardhooking)\n",
    "    \n",
    "    model(ori_imgs)\n",
    "    model(pgd_imgs)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_grad_cam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Comparing_featuremap's_similarity/comparing_whole_layer.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73775f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f70726d6c403136332e3135322e37342e3735227d7d/workspace/Comparing_featuremap%27s_similarity/comparing_whole_layer.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73775f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f70726d6c403136332e3135322e37342e3735227d7d/workspace/Comparing_featuremap%27s_similarity/comparing_whole_layer.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73775f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f70726d6c403136332e3135322e37342e3735227d7d/workspace/Comparing_featuremap%27s_similarity/comparing_whole_layer.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_grad_cam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m show_cam_on_image\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73775f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f70726d6c403136332e3135322e37342e3735227d7d/workspace/Comparing_featuremap%27s_similarity/comparing_whole_layer.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m resnet50\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73775f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f70726d6c403136332e3135322e37342e3735227d7d/workspace/Comparing_featuremap%27s_similarity/comparing_whole_layer.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPGD\u001b[39;00m \u001b[39mimport\u001b[39;00m PGD\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_grad_cam'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet50\n",
    "from PGD import PGD\n",
    "from my_resnet import *\n",
    "from models import *\n",
    "import os\n",
    "import torchvision\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1,2\"  # Set the GPUs 2 and 3 to use\n",
    "    \n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(device)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='/workspace/my_data', train=False, download=False, transform=transform_test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "model = ResNet18() ## original model\n",
    "checkpoint_path2 = '/workspace/checkpoint/resnet18_cifar10'\n",
    "checkpoint2 = torch.load(checkpoint_path2)['net']\n",
    "for key in list(checkpoint2.keys()):\n",
    "    checkpoint2[key.replace('module.', '')] = checkpoint2.pop(key)\n",
    "model.load_state_dict(checkpoint2, strict=False)## original model\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "\n",
    "robust_model = ResNet18()\n",
    "checkpoint_path1 = '/workspace/checkpoint/pgd_adversarial_training'\n",
    "checkpoint1 = torch.load(checkpoint_path1)['net']\n",
    "for key in list(checkpoint1.keys()):\n",
    "    checkpoint1[key.replace('module.', '')] = checkpoint1.pop(key)\n",
    "robust_model.load_state_dict(checkpoint1, strict=False)\n",
    "robust_model = robust_model.cuda()\n",
    "robust_model.eval()\n",
    "\n",
    "attack = PGD(model, cuda=cuda) # original model\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "max_dict = {}\n",
    "min_dict = {}\n",
    "\n",
    "for ori_imgs, ori_labels in test_loader:\n",
    "\n",
    "    pgd_imgs = attack(ori_imgs, ori_labels)\n",
    "    ori_imgs = ori_imgs.cuda()\n",
    "    # robust_pgd_imgs = attack1(ori_imgs, ori_labels)\n",
    "    # pgd_input_tensor = attack(ori_imgs, ori_labels)\n",
    "    # Note: input_tensor can be a batch tensor with several images!\n",
    "\n",
    "    holder = []\n",
    "            \n",
    "    def forwardhooking(m, inp, out):\n",
    "        holder.append(out)\n",
    "    # forwardhooking = ForwardHooking\n",
    "\n",
    "    for name, m in robust_model.named_modules():\n",
    "        if name == 'layer4.1':\n",
    "            m.register_forward_hook(forwardhooking)\n",
    "\n",
    "    robust_model(ori_imgs)\n",
    "    robust_model(pgd_imgs)\n",
    "    \n",
    "\n",
    "    # get cosine similarity along 512channel in last layer\n",
    "    sim = cos(holder[0][:,:,-1], holder[1][:,:,-1])\n",
    "    max_values, max_indcies = torch.topk(sim, 10, dim=1)\n",
    "    max_values = max_values.tolist()\n",
    "    max_indcies = max_indcies.squeeze(0).tolist()\n",
    "    min_values, min_indcies = torch.topk(sim, 10, dim=1, largest=False)\n",
    "    min_values = min_values.tolist()\n",
    "    min_indcies = min_indcies.squeeze(0).tolist()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(10):\n",
    "            if max_indcies[i][j] in max_dict:\n",
    "                max_dict[max_indcies[i][j]].append(max_values[i][j])\n",
    "            else:\n",
    "                max_dict[max_indcies[i][j]] = [max_values[i][j]]\n",
    "                \n",
    "            if min_indcies[i][j] in min_dict:\n",
    "                min_dict[min_indcies[i][j]].append(min_values[i][j])\n",
    "            else:\n",
    "                min_dict[min_indcies[i][j]] = [min_values[i][j]]\n",
    "    \n",
    "\n",
    "\n",
    "min_dict_sorted = sorted(min_dict.items(), key=lambda x: x[1], reverse=False)\n",
    "max_dict_sorted = sorted(max_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
